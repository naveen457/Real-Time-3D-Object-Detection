{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07e468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# Install pycocotools before running (uncomment when needed)\n",
    "# !pip install pycocotools\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8808f418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "# --- 1. SETUP AND DATA ACQUISITION ---\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "hf_token = \"#\"  ##your hugging face token \n",
    "api_url = \"https://datasets-server.huggingface.co/rows?dataset=rafaelpadilla%2Fcoco2017&config=default&split=train&offset=0&length=100\"\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Fetch metadata from HF\n",
    "# -----------------------------\n",
    "try:\n",
    "    response = requests.get(api_url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching data: {e}\")\n",
    "    exit()\n",
    "\n",
    "images_data = data.get(\"rows\", [])\n",
    "\n",
    "# folder for storing dataset images\n",
    "os.makedirs(\"coco_images\", exist_ok=True)\n",
    "\n",
    "dataset = []\n",
    "\n",
    "print(\"Processing images and annotations...\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Loop through HF metadata\n",
    "# -----------------------------\n",
    "for item in images_data:\n",
    "    row = item[\"row\"]\n",
    "\n",
    "    img_url = row[\"image\"][\"src\"]\n",
    "    img_id = row[\"image_id\"]\n",
    "    img_path = f\"coco_images/{img_id}.jpg\"\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # NEW: Option 1 → use local image if present\n",
    "    # ------------------------------------------\n",
    "    if os.path.exists(img_path):\n",
    "        print(f\"Found local image {img_id}. Using existing file.\")\n",
    "    else:\n",
    "        # ------------------------------------------\n",
    "        # Option 2 → download from Hugging Face\n",
    "        # ------------------------------------------\n",
    "        print(f\"Local image not found for {img_id}. Downloading from HF...\")\n",
    "        try:\n",
    "            img_response = requests.get(img_url, timeout=10)\n",
    "            img_response.raise_for_status()\n",
    "\n",
    "            img = Image.open(BytesIO(img_response.content)).convert(\"RGB\")\n",
    "            img.save(img_path)\n",
    "\n",
    "            print(f\"Downloaded and saved image {img_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download image {img_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Extract annotations\n",
    "    objects = row.get(\"objects\", {})\n",
    "    boxes = objects.get(\"bbox\", [])\n",
    "    labels = objects.get(\"label\", [])\n",
    "\n",
    "    dataset.append({\n",
    "        \"image_path\": img_path,\n",
    "        \"boxes\": boxes,\n",
    "        \"labels\": labels\n",
    "    })\n",
    "\n",
    "print(f\"Total images processed: {len(dataset)}\")\n",
    "\n",
    "# Keep only samples with valid boxes\n",
    "valid_dataset = [item for item in dataset if item.get(\"boxes\")]\n",
    "print(f\"Valid samples with annotations: {len(valid_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d634b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. CUSTOM DATASET CLASS ---\n",
    "\n",
    "class CustomCocoDataset(Dataset):\n",
    "    def __init__(self, dataset_list, transforms=None):\n",
    "        self.dataset_list = dataset_list\n",
    "        self.transforms = transforms if transforms is not None else T.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset_list[idx]\n",
    "        img = Image.open(data['image_path']).convert(\"RGB\")\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for box, label in zip(data['boxes'], data['labels']):\n",
    "            x, y, w, h = box\n",
    "            x_min, y_min = x, y\n",
    "            x_max, y_max = x + w, y + h\n",
    "\n",
    "            if w > 0 and h > 0 and x_max > x_min and y_max > y_min:\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(label)\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        img = self.transforms(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf9339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. DATASET SPLIT AND DATALOADER SETUP ---\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_size = int(0.2 * len(valid_dataset))\n",
    "train_size = len(valid_dataset) - val_size\n",
    "train_subset, val_subset = random_split(valid_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataset = CustomCocoDataset(train_subset, transforms=transform)\n",
    "val_dataset = CustomCocoDataset(val_subset, transforms=transform)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7982c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. MODEL SETUP ---\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "num_classes = max([label for d in valid_dataset for label in d['labels']]) + 1\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(f\"✅ Model modified for {num_classes} classes and moved to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183ae2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. EVALUATION FUNCTION ---\n",
    "\n",
    "def evaluate_coco(model, data_loader, device):\n",
    "    model.eval()\n",
    "    gt_annotations = []\n",
    "    dt_annotations = []\n",
    "    image_ids = []\n",
    "    ann_id = 1\n",
    "    img_id_counter = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            outputs = model(images)\n",
    "\n",
    "            for i, output in enumerate(outputs):\n",
    "                gt_boxes = targets[i]['boxes'].cpu().numpy()\n",
    "                gt_labels = targets[i]['labels'].cpu().numpy()\n",
    "                img_id = img_id_counter\n",
    "\n",
    "                for box, label in zip(gt_boxes, gt_labels):\n",
    "                    x_min, y_min, x_max, y_max = box\n",
    "                    width = x_max - x_min\n",
    "                    height = y_max - y_min\n",
    "                    area = width * height\n",
    "                    gt_annotations.append({\n",
    "                        \"image_id\": img_id,\n",
    "                        \"category_id\": int(label),\n",
    "                        \"bbox\": [float(x_min), float(y_min), float(width), float(height)],\n",
    "                        \"area\": float(area),\n",
    "                        \"id\": ann_id,\n",
    "                        \"iscrowd\": 0\n",
    "                    })\n",
    "                    ann_id += 1\n",
    "\n",
    "                pred_boxes = output['boxes'].cpu().numpy()\n",
    "                pred_scores = output['scores'].cpu().numpy()\n",
    "                pred_labels = output['labels'].cpu().numpy()\n",
    "                for p_box, p_score, p_label in zip(pred_boxes, pred_scores, pred_labels):\n",
    "                    x_min, y_min, x_max, y_max = p_box\n",
    "                    width = x_max - x_min\n",
    "                    height = y_max - y_min\n",
    "                    dt_annotations.append({\n",
    "                        \"image_id\": img_id,\n",
    "                        \"category_id\": int(p_label),\n",
    "                        \"bbox\": [float(x_min), float(y_min), float(width), float(height)],\n",
    "                        \"score\": float(p_score)\n",
    "                    })\n",
    "\n",
    "                image_ids.append(img_id)\n",
    "                img_id_counter += 1\n",
    "\n",
    "    coco_gt = COCO()\n",
    "    coco_gt.dataset = {\n",
    "     \"info\": {},   # Add empty info dictionary\n",
    "     \"licenses\": [],  # Add empty licenses list\n",
    "      \"images\": [{\"id\": img_id} for img_id in image_ids],\n",
    "     \"annotations\": gt_annotations,\n",
    "      \"categories\": [{\"id\": i} for i in range(num_classes)]\n",
    "    }\n",
    "    coco_gt.createIndex()\n",
    "\n",
    "    coco_gt.createIndex()\n",
    "\n",
    "    coco_dt = coco_gt.loadRes(dt_annotations)\n",
    "\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "    coco_eval.params.imgIds = image_ids\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    return coco_eval.stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8c5fbae",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'coco_images/254042.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     31\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     34\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(img\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[0;32m     35\u001b[0m     targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n",
      "File \u001b[1;32mc:\\Users\\navee\\Envs\\3D_object_detection_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    740\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\navee\\Envs\\3D_object_detection_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\navee\\Envs\\3D_object_detection_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\navee\\Envs\\3D_object_detection_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m, in \u001b[0;36mCustomCocoDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     12\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_list[idx]\n\u001b[1;32m---> 13\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m     labels \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\navee\\Envs\\3D_object_detection_env\\lib\\site-packages\\PIL\\Image.py:3513\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[0;32m   3512\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[1;32m-> 3513\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3514\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3515\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'coco_images/254042.jpg'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# --- 6. TRAINING LOOP WITH CHECKPOINT SAVING AND RESUME ---\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "num_epochs = 20\n",
    "\n",
    "checkpoint_dir = \"Models\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Check for existing checkpoints and load the latest if available\n",
    "checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n",
    "if checkpoint_files:\n",
    "    checkpoint_files.sort(key=lambda f: int(f.split('_')[-1].split('.')[0]))\n",
    "    latest_checkpoint_path = os.path.join(checkpoint_dir, checkpoint_files[-1])\n",
    "    checkpoint = torch.load(latest_checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    print(\"Starting training from scratch\")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += losses.item()\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Save checkpoint for each epoch\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"coco2017_{epoch+1}.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "        'loss': epoch_loss / len(train_loader)\n",
    "    }, checkpoint_path)\n",
    "    print(f\"✅ Saved checkpoint: {checkpoint_path}\")\n",
    "\n",
    "print(\"✅ Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca3a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best or final checkpoint\n",
    "checkpoint_path = os.path.join(checkpoint_dir, f\"coco2017_{max(int(f.split('_')[1].split('.')[0]) for f in os.listdir(checkpoint_dir) if f.startswith('coco2017_') and f.endswith('.pth'))}.pth\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad9f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Print the saved loss from this checkpoint\n",
    "saved_loss = checkpoint.get('loss', None)\n",
    "if saved_loss is not None:\n",
    "    print(f\"Training loss saved in checkpoint for epoch {checkpoint['epoch']}: {saved_loss:.4f}\")\n",
    "\n",
    "print(\"Starting final evaluation on validation set:\")\n",
    "metrics = evaluate_coco(model, val_loader, device)\n",
    "print(f\"Final Validation mAP (AP@[IoU=0.50:0.95]): {metrics[0]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3D_object_detection_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
