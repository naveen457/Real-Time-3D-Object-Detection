{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c07e468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# Install pycocotools before running (uncomment when needed)\n",
    "# !pip install pycocotools\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8808f418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images and annotations...\n",
      "Local image not found for 147328. Downloading from HF...\n",
      "Downloaded and saved image 147328\n",
      "Local image not found for 414738. Downloading from HF...\n",
      "Downloaded and saved image 414738\n",
      "Local image not found for 281563. Downloading from HF...\n",
      "Downloaded and saved image 281563\n",
      "Local image not found for 63879. Downloading from HF...\n",
      "Downloaded and saved image 63879\n",
      "Local image not found for 531349. Downloading from HF...\n",
      "Downloaded and saved image 531349\n",
      "Local image not found for 340329. Downloading from HF...\n",
      "Downloaded and saved image 340329\n",
      "Local image not found for 182236. Downloading from HF...\n",
      "Downloaded and saved image 182236\n",
      "Local image not found for 326820. Downloading from HF...\n",
      "Downloaded and saved image 326820\n",
      "Local image not found for 149364. Downloading from HF...\n",
      "Downloaded and saved image 149364\n",
      "Local image not found for 166598. Downloading from HF...\n",
      "Downloaded and saved image 166598\n",
      "Local image not found for 424792. Downloading from HF...\n",
      "Downloaded and saved image 424792\n",
      "Local image not found for 178388. Downloading from HF...\n",
      "Downloaded and saved image 178388\n",
      "Local image not found for 568154. Downloading from HF...\n",
      "Downloaded and saved image 568154\n",
      "Local image not found for 268150. Downloading from HF...\n",
      "Downloaded and saved image 268150\n",
      "Local image not found for 138473. Downloading from HF...\n",
      "Downloaded and saved image 138473\n",
      "Local image not found for 382699. Downloading from HF...\n",
      "Downloaded and saved image 382699\n",
      "Local image not found for 493952. Downloading from HF...\n",
      "Downloaded and saved image 493952\n",
      "Local image not found for 348522. Downloading from HF...\n",
      "Downloaded and saved image 348522\n",
      "Local image not found for 66862. Downloading from HF...\n",
      "Downloaded and saved image 66862\n",
      "Local image not found for 24457. Downloading from HF...\n",
      "Downloaded and saved image 24457\n",
      "Local image not found for 160437. Downloading from HF...\n",
      "Downloaded and saved image 160437\n",
      "Local image not found for 487414. Downloading from HF...\n",
      "Downloaded and saved image 487414\n",
      "Local image not found for 329541. Downloading from HF...\n",
      "Downloaded and saved image 329541\n",
      "Local image not found for 92064. Downloading from HF...\n",
      "Downloaded and saved image 92064\n",
      "Local image not found for 281262. Downloading from HF...\n",
      "Downloaded and saved image 281262\n",
      "Local image not found for 145337. Downloading from HF...\n",
      "Downloaded and saved image 145337\n",
      "Local image not found for 372147. Downloading from HF...\n",
      "Downloaded and saved image 372147\n",
      "Local image not found for 559184. Downloading from HF...\n",
      "Downloaded and saved image 559184\n",
      "Local image not found for 511141. Downloading from HF...\n",
      "Downloaded and saved image 511141\n",
      "Local image not found for 228974. Downloading from HF...\n",
      "Downloaded and saved image 228974\n",
      "Local image not found for 92658. Downloading from HF...\n",
      "Downloaded and saved image 92658\n",
      "Local image not found for 528299. Downloading from HF...\n",
      "Downloaded and saved image 528299\n",
      "Local image not found for 494811. Downloading from HF...\n",
      "Downloaded and saved image 494811\n",
      "Local image not found for 63873. Downloading from HF...\n",
      "Downloaded and saved image 63873\n",
      "Local image not found for 156073. Downloading from HF...\n",
      "Downloaded and saved image 156073\n",
      "Local image not found for 406342. Downloading from HF...\n",
      "Downloaded and saved image 406342\n",
      "Local image not found for 126288. Downloading from HF...\n",
      "Downloaded and saved image 126288\n",
      "Local image not found for 550547. Downloading from HF...\n",
      "Downloaded and saved image 550547\n",
      "Local image not found for 420414. Downloading from HF...\n",
      "Downloaded and saved image 420414\n",
      "Local image not found for 240755. Downloading from HF...\n",
      "Downloaded and saved image 240755\n",
      "Local image not found for 218357. Downloading from HF...\n",
      "Downloaded and saved image 218357\n",
      "Local image not found for 209073. Downloading from HF...\n",
      "Downloaded and saved image 209073\n",
      "Local image not found for 250607. Downloading from HF...\n",
      "Downloaded and saved image 250607\n",
      "Local image not found for 327818. Downloading from HF...\n",
      "Downloaded and saved image 327818\n",
      "Local image not found for 175111. Downloading from HF...\n",
      "Downloaded and saved image 175111\n",
      "Local image not found for 242755. Downloading from HF...\n",
      "Downloaded and saved image 242755\n",
      "Local image not found for 236173. Downloading from HF...\n",
      "Downloaded and saved image 236173\n",
      "Local image not found for 460918. Downloading from HF...\n",
      "Downloaded and saved image 460918\n",
      "Local image not found for 29716. Downloading from HF...\n",
      "Downloaded and saved image 29716\n",
      "Local image not found for 572289. Downloading from HF...\n",
      "Downloaded and saved image 572289\n",
      "Local image not found for 181356. Downloading from HF...\n",
      "Downloaded and saved image 181356\n",
      "Local image not found for 554271. Downloading from HF...\n",
      "Downloaded and saved image 554271\n",
      "Local image not found for 117770. Downloading from HF...\n",
      "Downloaded and saved image 117770\n",
      "Local image not found for 483760. Downloading from HF...\n",
      "Downloaded and saved image 483760\n",
      "Local image not found for 318929. Downloading from HF...\n",
      "Downloaded and saved image 318929\n",
      "Local image not found for 265544. Downloading from HF...\n",
      "Downloaded and saved image 265544\n",
      "Local image not found for 302823. Downloading from HF...\n",
      "Downloaded and saved image 302823\n",
      "Local image not found for 205477. Downloading from HF...\n",
      "Downloaded and saved image 205477\n",
      "Local image not found for 353536. Downloading from HF...\n",
      "Downloaded and saved image 353536\n",
      "Local image not found for 352660. Downloading from HF...\n",
      "Downloaded and saved image 352660\n",
      "Local image not found for 260566. Downloading from HF...\n",
      "Downloaded and saved image 260566\n",
      "Local image not found for 351903. Downloading from HF...\n",
      "Downloaded and saved image 351903\n",
      "Local image not found for 38226. Downloading from HF...\n",
      "Downloaded and saved image 38226\n",
      "Local image not found for 96539. Downloading from HF...\n",
      "Downloaded and saved image 96539\n",
      "Local image not found for 580121. Downloading from HF...\n",
      "Downloaded and saved image 580121\n",
      "Local image not found for 340218. Downloading from HF...\n",
      "Downloaded and saved image 340218\n",
      "Local image not found for 175102. Downloading from HF...\n",
      "Downloaded and saved image 175102\n",
      "Local image not found for 33300. Downloading from HF...\n",
      "Downloaded and saved image 33300\n",
      "Local image not found for 90488. Downloading from HF...\n",
      "Downloaded and saved image 90488\n",
      "Local image not found for 155782. Downloading from HF...\n",
      "Downloaded and saved image 155782\n",
      "Local image not found for 568878. Downloading from HF...\n",
      "Downloaded and saved image 568878\n",
      "Local image not found for 527480. Downloading from HF...\n",
      "Downloaded and saved image 527480\n",
      "Local image not found for 304292. Downloading from HF...\n",
      "Downloaded and saved image 304292\n",
      "Local image not found for 69121. Downloading from HF...\n",
      "Downloaded and saved image 69121\n",
      "Local image not found for 257341. Downloading from HF...\n",
      "Downloaded and saved image 257341\n",
      "Local image not found for 93279. Downloading from HF...\n",
      "Downloaded and saved image 93279\n",
      "Local image not found for 536738. Downloading from HF...\n",
      "Downloaded and saved image 536738\n",
      "Local image not found for 101159. Downloading from HF...\n",
      "Downloaded and saved image 101159\n",
      "Local image not found for 254042. Downloading from HF...\n",
      "Downloaded and saved image 254042\n",
      "Local image not found for 40870. Downloading from HF...\n",
      "Downloaded and saved image 40870\n",
      "Local image not found for 331282. Downloading from HF...\n",
      "Downloaded and saved image 331282\n",
      "Local image not found for 116502. Downloading from HF...\n",
      "Downloaded and saved image 116502\n",
      "Local image not found for 20063. Downloading from HF...\n",
      "Downloaded and saved image 20063\n",
      "Local image not found for 144717. Downloading from HF...\n",
      "Downloaded and saved image 144717\n",
      "Local image not found for 564088. Downloading from HF...\n",
      "Downloaded and saved image 564088\n",
      "Local image not found for 553051. Downloading from HF...\n",
      "Downloaded and saved image 553051\n",
      "Local image not found for 203372. Downloading from HF...\n",
      "Downloaded and saved image 203372\n",
      "Local image not found for 117093. Downloading from HF...\n",
      "Downloaded and saved image 117093\n",
      "Local image not found for 536419. Downloading from HF...\n",
      "Downloaded and saved image 536419\n",
      "Local image not found for 500152. Downloading from HF...\n",
      "Downloaded and saved image 500152\n",
      "Local image not found for 463771. Downloading from HF...\n",
      "Downloaded and saved image 463771\n",
      "Local image not found for 520799. Downloading from HF...\n",
      "Downloaded and saved image 520799\n",
      "Local image not found for 458103. Downloading from HF...\n",
      "Downloaded and saved image 458103\n",
      "Local image not found for 259712. Downloading from HF...\n",
      "Downloaded and saved image 259712\n",
      "Local image not found for 445643. Downloading from HF...\n",
      "Downloaded and saved image 445643\n",
      "Local image not found for 184155. Downloading from HF...\n",
      "Downloaded and saved image 184155\n",
      "Local image not found for 42404. Downloading from HF...\n",
      "Downloaded and saved image 42404\n",
      "Local image not found for 459184. Downloading from HF...\n",
      "Downloaded and saved image 459184\n",
      "Local image not found for 564602. Downloading from HF...\n",
      "Downloaded and saved image 564602\n",
      "Local image not found for 29161. Downloading from HF...\n",
      "Downloaded and saved image 29161\n",
      "Total images processed: 100\n",
      "Valid samples with annotations: 99\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "# --- 1. SETUP AND DATA ACQUISITION ---\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "hf_token = \"#\"  ##your hugging face token \n",
    "api_url = \"https://datasets-server.huggingface.co/rows?dataset=rafaelpadilla%2Fcoco2017&config=default&split=train&offset=0&length=100\"\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Fetch metadata from HF\n",
    "# -----------------------------\n",
    "try:\n",
    "    response = requests.get(api_url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching data: {e}\")\n",
    "    exit()\n",
    "\n",
    "images_data = data.get(\"rows\", [])\n",
    "\n",
    "# folder for storing dataset images\n",
    "os.makedirs(\"coco_images\", exist_ok=True)\n",
    "\n",
    "dataset = []\n",
    "\n",
    "print(\"Processing images and annotations...\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Loop through HF metadata\n",
    "# -----------------------------\n",
    "for item in images_data:\n",
    "    row = item[\"row\"]\n",
    "\n",
    "    img_url = row[\"image\"][\"src\"]\n",
    "    img_id = row[\"image_id\"]\n",
    "    img_path = f\"coco_images/{img_id}.jpg\"\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # NEW: Option 1 → use local image if present\n",
    "    # ------------------------------------------\n",
    "    if os.path.exists(img_path):\n",
    "        print(f\"Found local image {img_id}. Using existing file.\")\n",
    "    else:\n",
    "        # ------------------------------------------\n",
    "        # Option 2 → download from Hugging Face\n",
    "        # ------------------------------------------\n",
    "        print(f\"Local image not found for {img_id}. Downloading from HF...\")\n",
    "        try:\n",
    "            img_response = requests.get(img_url, timeout=10)\n",
    "            img_response.raise_for_status()\n",
    "\n",
    "            img = Image.open(BytesIO(img_response.content)).convert(\"RGB\")\n",
    "            img.save(img_path)\n",
    "\n",
    "            print(f\"Downloaded and saved image {img_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download image {img_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Extract annotations\n",
    "    objects = row.get(\"objects\", {})\n",
    "    boxes = objects.get(\"bbox\", [])\n",
    "    labels = objects.get(\"label\", [])\n",
    "\n",
    "    dataset.append({\n",
    "        \"image_path\": img_path,\n",
    "        \"boxes\": boxes,\n",
    "        \"labels\": labels\n",
    "    })\n",
    "\n",
    "print(f\"Total images processed: {len(dataset)}\")\n",
    "\n",
    "# Keep only samples with valid boxes\n",
    "valid_dataset = [item for item in dataset if item.get(\"boxes\")]\n",
    "print(f\"Valid samples with annotations: {len(valid_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d634b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. CUSTOM DATASET CLASS ---\n",
    "\n",
    "class CustomCocoDataset(Dataset):\n",
    "    def __init__(self, dataset_list, transforms=None):\n",
    "        self.dataset_list = dataset_list\n",
    "        self.transforms = transforms if transforms is not None else T.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset_list[idx]\n",
    "        img = Image.open(data['image_path']).convert(\"RGB\")\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for box, label in zip(data['boxes'], data['labels']):\n",
    "            x, y, w, h = box\n",
    "            x_min, y_min = x, y\n",
    "            x_max, y_max = x + w, y + h\n",
    "\n",
    "            if w > 0 and h > 0 and x_max > x_min and y_max > y_min:\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(label)\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        img = self.transforms(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7edf9339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. DATASET SPLIT AND DATALOADER SETUP ---\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_size = int(0.2 * len(valid_dataset))\n",
    "train_size = len(valid_dataset) - val_size\n",
    "train_subset, val_subset = random_split(valid_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataset = CustomCocoDataset(train_subset, transforms=transform)\n",
    "val_dataset = CustomCocoDataset(val_subset, transforms=transform)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7982c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to C:\\Users\\navee/.cache\\torch\\hub\\checkpoints\\fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 23.9M/160M [00:40<02:03, 1.15MB/s]"
     ]
    }
   ],
   "source": [
    "# --- 4. MODEL SETUP ---\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "num_classes = max([label for d in valid_dataset for label in d['labels']]) + 1\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(f\"✅ Model modified for {num_classes} classes and moved to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183ae2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. EVALUATION FUNCTION ---\n",
    "\n",
    "def evaluate_coco(model, data_loader, device):\n",
    "    model.eval()\n",
    "    gt_annotations = []\n",
    "    dt_annotations = []\n",
    "    image_ids = []\n",
    "    ann_id = 1\n",
    "    img_id_counter = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            outputs = model(images)\n",
    "\n",
    "            for i, output in enumerate(outputs):\n",
    "                gt_boxes = targets[i]['boxes'].cpu().numpy()\n",
    "                gt_labels = targets[i]['labels'].cpu().numpy()\n",
    "                img_id = img_id_counter\n",
    "\n",
    "                for box, label in zip(gt_boxes, gt_labels):\n",
    "                    x_min, y_min, x_max, y_max = box\n",
    "                    width = x_max - x_min\n",
    "                    height = y_max - y_min\n",
    "                    area = width * height\n",
    "                    gt_annotations.append({\n",
    "                        \"image_id\": img_id,\n",
    "                        \"category_id\": int(label),\n",
    "                        \"bbox\": [float(x_min), float(y_min), float(width), float(height)],\n",
    "                        \"area\": float(area),\n",
    "                        \"id\": ann_id,\n",
    "                        \"iscrowd\": 0\n",
    "                    })\n",
    "                    ann_id += 1\n",
    "\n",
    "                pred_boxes = output['boxes'].cpu().numpy()\n",
    "                pred_scores = output['scores'].cpu().numpy()\n",
    "                pred_labels = output['labels'].cpu().numpy()\n",
    "                for p_box, p_score, p_label in zip(pred_boxes, pred_scores, pred_labels):\n",
    "                    x_min, y_min, x_max, y_max = p_box\n",
    "                    width = x_max - x_min\n",
    "                    height = y_max - y_min\n",
    "                    dt_annotations.append({\n",
    "                        \"image_id\": img_id,\n",
    "                        \"category_id\": int(p_label),\n",
    "                        \"bbox\": [float(x_min), float(y_min), float(width), float(height)],\n",
    "                        \"score\": float(p_score)\n",
    "                    })\n",
    "\n",
    "                image_ids.append(img_id)\n",
    "                img_id_counter += 1\n",
    "\n",
    "    coco_gt = COCO()\n",
    "    coco_gt.dataset = {\n",
    "     \"info\": {},   # Add empty info dictionary\n",
    "     \"licenses\": [],  # Add empty licenses list\n",
    "      \"images\": [{\"id\": img_id} for img_id in image_ids],\n",
    "     \"annotations\": gt_annotations,\n",
    "      \"categories\": [{\"id\": i} for i in range(num_classes)]\n",
    "    }\n",
    "    coco_gt.createIndex()\n",
    "\n",
    "    coco_gt.createIndex()\n",
    "\n",
    "    coco_dt = coco_gt.loadRes(dt_annotations)\n",
    "\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "    coco_eval.params.imgIds = image_ids\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    return coco_eval.stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c5fbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 12\n",
      "Starting training...\n",
      "✅ Training complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# --- 6. TRAINING LOOP WITH CHECKPOINT SAVING AND RESUME ---\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "num_epochs = 12\n",
    "\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Check for existing checkpoints and load the latest if available\n",
    "checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n",
    "if checkpoint_files:\n",
    "    checkpoint_files.sort(key=lambda f: int(f.split('_')[-1].split('.')[0]))\n",
    "    latest_checkpoint_path = os.path.join(checkpoint_dir, checkpoint_files[-1])\n",
    "    checkpoint = torch.load(latest_checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    print(\"Starting training from scratch\")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += losses.item()\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Save checkpoint for each epoch\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"fasterrcnn_epoch_{epoch+1}.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "        'loss': epoch_loss / len(train_loader)\n",
    "    }, checkpoint_path)\n",
    "    print(f\"✅ Saved checkpoint: {checkpoint_path}\")\n",
    "\n",
    "print(\"✅ Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca3a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best or final checkpoint\n",
    "checkpoint_path = os.path.join(checkpoint_dir, f\"fasterrcnn_epoch_{2}.pth\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad9f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss saved in checkpoint for epoch 2: 1.0425\n",
      "Starting final evaluation on validation set:\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.17s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.38s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.004\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.003\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.007\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.011\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.018\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.023\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.020\n",
      "Final Validation mAP (AP@[IoU=0.50:0.95]): 0.0014\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Print the saved loss from this checkpoint\n",
    "saved_loss = checkpoint.get('loss', None)\n",
    "if saved_loss is not None:\n",
    "    print(f\"Training loss saved in checkpoint for epoch {checkpoint['epoch']}: {saved_loss:.4f}\")\n",
    "\n",
    "print(\"Starting final evaluation on validation set:\")\n",
    "metrics = evaluate_coco(model, val_loader, device)\n",
    "print(f\"Final Validation mAP (AP@[IoU=0.50:0.95]): {metrics[0]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3D_object_detection_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
