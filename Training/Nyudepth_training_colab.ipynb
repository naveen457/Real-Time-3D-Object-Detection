{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz0KxZYpm8WI"
      },
      "outputs": [],
      "source": [
        "# --- SETUP ---\n",
        "%pip install datasets webdataset torch torchvision matplotlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as T\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlBPZGsiP1oq"
      },
      "outputs": [],
      "source": [
        "# 2. Login to Hugging Face\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(\"#\")   #  paste your HuggingFace token here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ad39YgsRJ9Z"
      },
      "outputs": [],
      "source": [
        "# --- TRANSFORMS ---\n",
        "transform_img = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.ToTensor()    # scales image to [0,1], shape [C,H,W]\n",
        "])\n",
        "transform_depth = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.ToTensor()     # converts depth np.array → tensor, shape [1,H,W]\n",
        "])\n",
        "\n",
        "\n",
        "def preprocess(example):\n",
        "    # RGB image (PIL)\n",
        "    img = example[\"jpg\"].convert(\"RGB\")\n",
        "    img = transform_img(img)\n",
        "\n",
        "    # Depth map (NumPy array)\n",
        "    depth = example[\"depth.npy\"]\n",
        "\n",
        "    # Convert to tensor (float32) and resize\n",
        "    depth = torch.tensor(depth, dtype=torch.float32)  # shape [H,W]\n",
        "    depth = depth.unsqueeze(0)  # add channel dim → [1,H,W]\n",
        "    depth = T.functional.resize(depth, (224, 224))   # resize to match image\n",
        "\n",
        "    # Replace in dict\n",
        "    example[\"image\"] = img\n",
        "    example[\"depth\"] = depth\n",
        "    return example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn0-sxCOZ1Aw"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Stream instead of downloading\n",
        "dataset = load_dataset(\"adams-story/nyu-depthv2-wds\", split=\"train\", streaming=True)\n",
        "\n",
        "train_data = dataset.take(500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aTq7LGwZ64n"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Peek at one sample\n",
        "sample = next(iter(train_data))\n",
        "sample=preprocess(sample)\n",
        "\n",
        "print(sample.keys())        # should show dict_keys(['image', 'depth', ...])\n",
        "print(sample[\"image\"].shape, sample[\"depth\"].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6D8hax3SRSP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Wrap into iterable\n",
        "def collate_fn(batch):\n",
        "    imgs, depths = [], []\n",
        "    for b in batch:\n",
        "        sample = preprocess(b)   # returns {\"image\": tensor, \"depth\": tensor}\n",
        "        imgs.append(sample[\"image\"])\n",
        "        depths.append(sample[\"depth\"])\n",
        "    return torch.stack(imgs), torch.stack(depths)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, collate_fn=collate_fn,num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gj_2hNT8SbhK"
      },
      "outputs": [],
      "source": [
        "# --- MODEL (simple U-Net style encoder-decoder) ---\n",
        "class DepthEstimationNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, stride=2, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64,128,3,stride=2,padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(128,256,3,stride=2,padding=1), nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256,128,3,stride=2,output_padding=1,padding=1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128,64,3,stride=2,output_padding=1,padding=1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64,1,3,stride=2,output_padding=1,padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        out = self.decoder(z)\n",
        "        return out\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = DepthEstimationNet().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwGxRhalWLAy"
      },
      "outputs": [],
      "source": [
        "# --- TRAINING ---\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.L1Loss()  # Mean Absolute Error\n",
        "\n",
        "EPOCHS = 15\n",
        "import time\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        batch_start = time.time()\n",
        "        imgs, depths = batch\n",
        "        imgs, depths = imgs.to(device), depths.to(device)\n",
        "        preds = model(imgs)\n",
        "        loss = loss_fn(preds, depths)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        batch_duration = time.time() - batch_start\n",
        "        print(f\"Batch {i+1} processing time: {batch_duration:.2f}s\")\n",
        "\n",
        "    epoch_duration = time.time() - start_time\n",
        "    avg_loss = total_loss / (i + 1)\n",
        "    print(f\"Epoch {epoch+1} done in {epoch_duration:.2f}s - Average Loss: {avg_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqHYAgwdeHwM"
      },
      "outputs": [],
      "source": [
        "# Save final trained model after all epochs\n",
        "torch.save(model.state_dict(), \"NYUDEPTH.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEIShbjBeS5W"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/NYUDEPTH.pt\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
